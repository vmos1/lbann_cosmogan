{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from output files\n",
    "### Analyze the output from a single LBANN run\n",
    "March 9, 2020 \\\n",
    "April 6, 2020 : to store files in order of epochs \\\n",
    "April 21, 2020: added jupyter widgets to compare pixel intensity plots \\\n",
    "May 8, 2020: using all images for a given batch \\\n",
    "May 29, 2020: Modified for new update of LBANN. File names of images changed, so new extraction code. Also added code for computing chi-squared. \\\n",
    "June 17, 2020: Removed train_inp, train_gen and val_inp to reduce memory overhead. From now on, the code only analyzes val_gen \\\n",
    "June 26, 2020: Added gathering of steps and new chi-square quantities.\\\n",
    "July 1, 2020: Switched back to storing mainly train_gen with large steps (10 steps saved for 256 batchsize).\\\n",
    "July 29, 2020: Perform analysis without storing images. Store histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess as sp\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import itertools\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/global/u1/v/vpa/project/jpt_notebooks/Cosmology/Cosmo_GAN/repositories/lbann_cosmogan/3_analysis')\n",
    "from modules_image_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4. + 1e-8) - 1.\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules for Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_sorted_df(main_dir):\n",
    "    \n",
    "    '''\n",
    "    Module to create Dataframe with filenames for each epoch and step\n",
    "    Sorts by step and epoch\n",
    "    '''\n",
    "    def f_get_info_from_fname(fname):\n",
    "        ''' Read file and return dictionary with epoch, step'''\n",
    "        dict1={}\n",
    "        dict1['epoch']=np.int32(fname.split('epoch')[-1].split('.')[1])\n",
    "        dict1['step']=np.int64(fname.split('step')[-1].split('.')[1].split('_')[0])\n",
    "        return dict1\n",
    "    \n",
    "    t1=time.time()\n",
    "    ### get list of file names\n",
    "    fldr_loc=main_dir+'/dump_outs/trainer0/model0/'\n",
    "#        keys=['train_gen','train_input','val_gen','val_input']\n",
    "#     file_strg_dict={'train_gen': sgd.training*_gen_img*_output0.npy','train_input':sgd.training*_inp_img*_output0.npy','val_gen':sgd.validation*_gen_img*_output0.npy','val_input':sgd.validation*_inp_img*_output0.npy'}\n",
    "    file_strg_dict={'train_gen':'sgd.training*_gen_img*_output0.npy'}\n",
    "    keys=['train_gen']\n",
    "\n",
    "    files_arr,img_arr=np.array([]),np.array([])\n",
    "    for key in keys:\n",
    "        print(key)\n",
    "        files=glob.glob(fldr_loc+file_strg_dict[key])\n",
    "        files_arr=np.append(files_arr,files)\n",
    "        img_arr=np.append(img_arr,[key] *len(files))\n",
    "\n",
    "    print('Number of files',len(files_arr))\n",
    "#     files_arr=np.array([glob.glob(fldr_loc+file_strg_dict[key]) for key in keys][0])\n",
    "\n",
    "    ### Create dataframe\n",
    "    df_files=pd.DataFrame()\n",
    "    df_files['img_type']=np.array(img_arr)\n",
    "    df_files['fname']=np.array(files_arr).astype(str)\n",
    "\n",
    "    # Create list of dictionaries\n",
    "    dict1=df_files.apply(lambda row : f_get_info_from_fname(row.fname),axis=1)\n",
    "    keys=dict1[0].keys() # Extract keys of dictionary\n",
    "    # print(keys)\n",
    "    # ### Convert list of dicts to dict of lists\n",
    "    dict_list={key:[k[key] for k in dict1] for key in keys}\n",
    "    # ### Add columns to Dataframe\n",
    "    for key in dict_list.keys():\n",
    "        df_files[key]=dict_list[key]\n",
    "\n",
    "    df_files=df_files.sort_values(by=['img_type','epoch','step']).reset_index(drop=True) ### sort df by epoch and step\n",
    "    \n",
    "    t2=time.time()\n",
    "    print(\"time for sorting\",t2-t1)\n",
    "\n",
    "    return df_files[['epoch','step','img_type','fname']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_compute_hist_spect(sample,bins):\n",
    "    ''' Compute pixel intensity histograms and radial spectrum for 2D arrays\n",
    "    Input : Image arrays and bins\n",
    "    Output: dictionary with 5 arrays : Histogram values, errors and bin centers, Spectrum values and errors.\n",
    "    '''\n",
    "    ### Compute pixel histogram for row\n",
    "    gen_hist,gen_err,hist_bins=f_batch_histogram(sample,bins=bins,norm=True,hist_range=None)\n",
    "    ### Compute spectrum for row\n",
    "    spec,spec_err=f_compute_spectrum(sample,plot=False)\n",
    "\n",
    "    dict1={'hist_val':gen_hist,'hist_err':gen_err,'hist_bin_centers':hist_bins,'spec_val':spec,'spec_err':spec_err }\n",
    "    return dict1\n",
    "\n",
    "def f_get_images(fname,img_type):\n",
    "    '''\n",
    "    Extract image using file name\n",
    "    '''\n",
    "    fname,key=fname,img_type\n",
    "    a1=np.load(fname)\n",
    "    if key.endswith('input'): \n",
    "        size=np.int(np.sqrt(a1.shape[-1])) ### Extract size of images (=128)\n",
    "        batch_size=a1.shape[0] ### Number of batches\n",
    "        samples=a1.reshape(batch_size,size,size)\n",
    "    elif key.endswith('gen') : samples=a1[:,0,:,:]\n",
    "    else : raise SystemError\n",
    "    \n",
    "    return samples\n",
    "    \n",
    "def f_high_pixel(images,cutoff=0.9966):\n",
    "    '''\n",
    "    Get number of images with a pixel about max cut-off value\n",
    "    '''\n",
    "    max_arr=np.amax(images,axis=(1,2))\n",
    "    num_large=max_arr[max_arr>cutoff].shape[0]\n",
    "\n",
    "    return num_large\n",
    "\n",
    "def f_compute_chisqr(dict_val,dict_sample):\n",
    "    '''\n",
    "    Compute chi-square values for sample w.r.t input images\n",
    "    Input: 2 dictionaries with 4 keys for histogram and spectrum values and errors\n",
    "    '''\n",
    "    ### !!Both pixel histograms MUST have same bins and normalization!\n",
    "    ### Compute chi-sqr\n",
    "    ### Used in keras code : np.sum(np.divide(np.power(valhist - samphist, 2.0), valhist))\n",
    "    ###  chi_sqr :: sum((Obs-Val)^2/(Val))\n",
    "    \n",
    "    chisqr_dict={}\n",
    "    \n",
    "    val_dr=dict_val['hist_val'].copy()\n",
    "    val_dr[val_dr<=0.]=1.0    ### Avoiding division by zero for zero bins\n",
    "    \n",
    "    sq_diff=(dict_val['hist_val']-dict_sample['hist_val'])**2\n",
    "    \n",
    "    size=len(dict_val['hist_val'])\n",
    "    l1,l2=int(size*0.3),int(size*0.7)\n",
    "    keys=['chi_1a','chi_1b','chi_1c','chi_1']\n",
    "    \n",
    "    for (key,start,end) in zip(keys,[0,l1,l2,0],[l1,l2,None,None]):  # 4 lists : small, medium, large pixel values and full \n",
    "        chisqr_dict.update({key:np.sum(np.divide(sq_diff[start:end],val_dr[start:end]))})\n",
    "    \n",
    "    idx=None  # Choosing the number of histograms to use. Eg : -5 to skip last 5 bins\n",
    "#     chisqr_dict.update({'chi_sqr1':})\n",
    "    \n",
    "    chisqr_dict.update({'chi_2':np.sum(np.divide(sq_diff[:idx],1.0))}) ## chi-sqr without denominator division\n",
    "    chisqr_dict.update({'chi_imgvar':np.sum(dict_sample['hist_err'][:idx])/np.sum(dict_val['hist_err'][:idx])}) ## measures total spread in histograms wrt to input data\n",
    "    \n",
    "    idx=60\n",
    "    spec_diff=(dict_val['spec_val']-dict_sample['spec_val'])**2\n",
    "    ### computing the spectral loss chi-square\n",
    "    chisqr_dict.update({'chi_spec1':np.sum(spec_diff[:idx]/dict_sample['spec_val'][:idx]**2)})\n",
    "    \n",
    "    ### computing the spectral loss chi-square\n",
    "    chisqr_dict.update({'chi_spec2':np.sum(spec_diff[:idx]/dict_sample['spec_err'][:idx]**2)})\n",
    "    \n",
    "    return chisqr_dict\n",
    "\n",
    "    \n",
    "def f_get_computed_dict(fname,img_type,bins,dict_val):\n",
    "    '''\n",
    "    Get dictionary with chisquare values and histogram and spectrum lists\n",
    "    '''\n",
    "    \n",
    "    ### Get images from file\n",
    "    images=f_get_images(fname,img_type)    \n",
    "    ### Compute high pixel values\n",
    "    high_pixel=f_high_pixel(images,cutoff=0.9898)\n",
    "    ### Compute spectrum and histograms\n",
    "    dict_sample=f_compute_hist_spect(images,bins) ## list of 5 numpy arrays \n",
    "    ### Compute chi squares\n",
    "    dict_chisqrs=f_compute_chisqr(dict_val,dict_sample)\n",
    "    \n",
    "    dict1={}\n",
    "    dict1.update(dict_chisqrs)\n",
    "    dict1.update({'num_large':high_pixel})\n",
    "    dict1.update(dict_sample)\n",
    "    \n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200725_172458_batchsize_64/\n"
     ]
    }
   ],
   "source": [
    "# fldr_name='20200718_114324_batchsize_512'\n",
    "# fldr_name='20200718_135530_batchsize_256'\n",
    "# fldr_name='20200725_204329_batchsize_256'\n",
    "fldr_name='20200725_172458_batchsize_64'\n",
    "# fldr_name='20200803_055550_batchsize_256'\n",
    "fldr_name='20200804_152954_batchsize_256/'\n",
    "\n",
    "main_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/{0}'.format(fldr_name)\n",
    "print(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_gen\n",
      "63440 63440\n",
      "time for sorting 3.073791027069092\n",
      "(63440, 4)\n"
     ]
    }
   ],
   "source": [
    "### Get dataframe with file names, sorted by epoch and step\n",
    "df_files=f_get_sorted_df(main_dir)\n",
    "print(df_files.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "### Extract validation data\n",
    "fname='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_2_smoothing_200k/norm_1_train_val.npy'\n",
    "s_val=np.load(fname,mmap_mode='r')[:8000][:,0,:,:]\n",
    "print(s_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "\n",
    "transform=False ## Images are in transformed space (-1,1), convert bins to the same space\n",
    "bins=np.concatenate([np.array([-0.5]),np.arange(0.5,20.5,1),np.arange(20.5,100.5,5),np.arange(100.5,1000.5,50),np.array([2000])]) #bin edges to use\n",
    "# bins=np.concatenate([np.array([-0.5]),np.arange(0.5,20.5,5),np.arange(20.5,100.5,20),np.arange(100.5,1000.5,100),np.array([2000])]) #bin edges to use\n",
    "if not transform: bins=f_transform(bins)   ### scale to (-1,1) \n",
    "### Compute histogram and spectrum of raw data \n",
    "dict_val=f_compute_hist_spect(s_val,bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial CPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_files.copy().head(10)\n",
    "\n",
    "t2=time.time()\n",
    "dict1=df.apply(lambda row: f_get_computed_dict(fname=row.fname,img_type='train_gen',bins=bins,dict_val=dict_val),axis=1)\n",
    "keys=dict1[0].keys()\n",
    "### Convert list of dicts to dict of lists\n",
    "dict_list={key:[k[key] for k in dict1] for key in keys}\n",
    "### Add columns to Dataframe\n",
    "for key in dict_list.keys():\n",
    "    df[key]=dict_list[key]\n",
    "    \n",
    "t3=time.time()\n",
    "print(\"Time \",t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel CPU test\n",
    "Using pandarallel : https://stackoverflow.com/questions/26784164/pandas-multiprocessing-apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0b4195e27d437fb116a9c425418dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['chi_1a', 'chi_1b', 'chi_1c', 'chi_1', 'chi_2', 'chi_imgvar', 'chi_spec1', 'chi_spec2', 'num_large', 'hist_val', 'hist_err', 'hist_bin_centers', 'spec_val', 'spec_err'])\n",
      "Time  1.446944236755371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>img_type</th>\n",
       "      <th>fname</th>\n",
       "      <th>chi_1a</th>\n",
       "      <th>chi_1b</th>\n",
       "      <th>chi_1c</th>\n",
       "      <th>chi_1</th>\n",
       "      <th>chi_2</th>\n",
       "      <th>chi_imgvar</th>\n",
       "      <th>chi_spec1</th>\n",
       "      <th>chi_spec2</th>\n",
       "      <th>num_large</th>\n",
       "      <th>hist_val</th>\n",
       "      <th>hist_err</th>\n",
       "      <th>hist_bin_centers</th>\n",
       "      <th>spec_val</th>\n",
       "      <th>spec_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>189.955298</td>\n",
       "      <td>4.778167</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>194.746651</td>\n",
       "      <td>12.417616</td>\n",
       "      <td>21.439721</td>\n",
       "      <td>18236.350265</td>\n",
       "      <td>3.549975e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.00021028518730998038, 0.058279842228064906,...</td>\n",
       "      <td>[4.0617537070536625e-05, 0.001978581126841826,...</td>\n",
       "      <td>[-1.031746031584782, -0.6161616169043975, -0.3...</td>\n",
       "      <td>[56690758.12141779, 2306.343328230083, 1060.85...</td>\n",
       "      <td>[41895.07346110348, 69.45253457053425, 36.8556...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>189.319593</td>\n",
       "      <td>6.005049</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>195.337827</td>\n",
       "      <td>12.170600</td>\n",
       "      <td>22.901184</td>\n",
       "      <td>18164.160802</td>\n",
       "      <td>3.532505e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0003285706051718441, 0.06978061805983715, 0...</td>\n",
       "      <td>[4.1581796383402746e-05, 0.0024182402181228547...</td>\n",
       "      <td>[-1.031746031584782, -0.6161616169043975, -0.3...</td>\n",
       "      <td>[56823907.517639935, 2174.3364949659444, 980.9...</td>\n",
       "      <td>[41871.344270795744, 89.13511950909188, 33.405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>174.543797</td>\n",
       "      <td>4.840065</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>179.397048</td>\n",
       "      <td>11.733181</td>\n",
       "      <td>22.045882</td>\n",
       "      <td>36208.258443</td>\n",
       "      <td>6.842088e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0002290606504626572, 0.07263958465321514, 0...</td>\n",
       "      <td>[2.8931124514245006e-05, 0.0026939925494827064...</td>\n",
       "      <td>[-1.031746031584782, -0.6161616169043975, -0.3...</td>\n",
       "      <td>[58477676.14610486, 778.594840059377, 658.1762...</td>\n",
       "      <td>[25157.625568626616, 32.80743395783755, 24.049...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>155.529899</td>\n",
       "      <td>4.091530</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>159.634615</td>\n",
       "      <td>11.026773</td>\n",
       "      <td>17.925114</td>\n",
       "      <td>36735.654257</td>\n",
       "      <td>5.885497e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0020108521036516867, 0.10408231632053087, 0...</td>\n",
       "      <td>[0.00011222143762225864, 0.002870345820699504,...</td>\n",
       "      <td>[-1.031746031584782, -0.6161616169043975, -0.3...</td>\n",
       "      <td>[60850329.67851047, 819.156503082253, 644.8699...</td>\n",
       "      <td>[32182.221657235295, 35.85308697820658, 21.235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>138.318907</td>\n",
       "      <td>3.584549</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>141.916641</td>\n",
       "      <td>10.247189</td>\n",
       "      <td>17.516094</td>\n",
       "      <td>15366.733078</td>\n",
       "      <td>2.979424e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.006141453997240586, 0.12620168943771828, 0....</td>\n",
       "      <td>[0.0003093124891087808, 0.003087684381940943, ...</td>\n",
       "      <td>[-1.031746031584782, -0.6161616169043975, -0.3...</td>\n",
       "      <td>[63578109.04957635, 3592.258982752144, 1342.44...</td>\n",
       "      <td>[77600.30036902921, 97.77704891208928, 40.8790...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  step   img_type                                              fname  \\\n",
       "0      0     0  train_gen  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...   \n",
       "1      0     1  train_gen  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...   \n",
       "2      0     2  train_gen  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...   \n",
       "3      0     3  train_gen  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...   \n",
       "4      0     4  train_gen  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...   \n",
       "\n",
       "       chi_1a    chi_1b    chi_1c       chi_1      chi_2  chi_imgvar  \\\n",
       "0  189.955298  4.778167  0.013186  194.746651  12.417616   21.439721   \n",
       "1  189.319593  6.005049  0.013186  195.337827  12.170600   22.901184   \n",
       "2  174.543797  4.840065  0.013186  179.397048  11.733181   22.045882   \n",
       "3  155.529899  4.091530  0.013186  159.634615  11.026773   17.925114   \n",
       "4  138.318907  3.584549  0.013186  141.916641  10.247189   17.516094   \n",
       "\n",
       "      chi_spec1     chi_spec2  num_large  \\\n",
       "0  18236.350265  3.549975e+07          0   \n",
       "1  18164.160802  3.532505e+07          0   \n",
       "2  36208.258443  6.842088e+07          0   \n",
       "3  36735.654257  5.885497e+07          0   \n",
       "4  15366.733078  2.979424e+07          0   \n",
       "\n",
       "                                            hist_val  \\\n",
       "0  [0.00021028518730998038, 0.058279842228064906,...   \n",
       "1  [0.0003285706051718441, 0.06978061805983715, 0...   \n",
       "2  [0.0002290606504626572, 0.07263958465321514, 0...   \n",
       "3  [0.0020108521036516867, 0.10408231632053087, 0...   \n",
       "4  [0.006141453997240586, 0.12620168943771828, 0....   \n",
       "\n",
       "                                            hist_err  \\\n",
       "0  [4.0617537070536625e-05, 0.001978581126841826,...   \n",
       "1  [4.1581796383402746e-05, 0.0024182402181228547...   \n",
       "2  [2.8931124514245006e-05, 0.0026939925494827064...   \n",
       "3  [0.00011222143762225864, 0.002870345820699504,...   \n",
       "4  [0.0003093124891087808, 0.003087684381940943, ...   \n",
       "\n",
       "                                    hist_bin_centers  \\\n",
       "0  [-1.031746031584782, -0.6161616169043975, -0.3...   \n",
       "1  [-1.031746031584782, -0.6161616169043975, -0.3...   \n",
       "2  [-1.031746031584782, -0.6161616169043975, -0.3...   \n",
       "3  [-1.031746031584782, -0.6161616169043975, -0.3...   \n",
       "4  [-1.031746031584782, -0.6161616169043975, -0.3...   \n",
       "\n",
       "                                            spec_val  \\\n",
       "0  [56690758.12141779, 2306.343328230083, 1060.85...   \n",
       "1  [56823907.517639935, 2174.3364949659444, 980.9...   \n",
       "2  [58477676.14610486, 778.594840059377, 658.1762...   \n",
       "3  [60850329.67851047, 819.156503082253, 644.8699...   \n",
       "4  [63578109.04957635, 3592.258982752144, 1342.44...   \n",
       "\n",
       "                                            spec_err  \n",
       "0  [41895.07346110348, 69.45253457053425, 36.8556...  \n",
       "1  [41871.344270795744, 89.13511950909188, 33.405...  \n",
       "2  [25157.625568626616, 32.80743395783755, 24.049...  \n",
       "3  [32182.221657235295, 35.85308697820658, 21.235...  \n",
       "4  [77600.30036902921, 97.77704891208928, 40.8790...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_files.copy().head(20)\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "t2=time.time()\n",
    "dict1=df.parallel_apply(lambda row: f_get_computed_dict(fname=row.fname,img_type='train_gen',bins=bins,dict_val=dict_val),axis=1)\n",
    "keys=dict1[0].keys()\n",
    "print(keys)\n",
    "### Convert list of dicts to dict of lists\n",
    "dict_list={key:[k[key] for k in dict1] for key in keys}\n",
    "### Add columns to Dataframe\n",
    "for key in dict_list.keys():\n",
    "    df[key]=dict_list[key]\n",
    "    \n",
    "t3=time.time()\n",
    "print(\"Time \",t3-t2)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize()\n",
    "# def func(x):\n",
    "#     return np.sin(x**2)\n",
    "\n",
    "# df.parallel_apply(func,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import cudf; print('cuDF Version:', cudf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def f_get_computed_dict_gpu(fname,img_type,num_large,dict_val,bins):\n",
    "#     '''\n",
    "#     '''\n",
    "    \n",
    "#     ### Get images from file\n",
    "#     for i,(_fname,_img_type) in enumerate(zip(fname,img_type)):\n",
    "#         images=f_get_images(_fname,_img_type)    \n",
    "#         ### Compute high pixel values\n",
    "#         high_pixel=f_high_pixel(images,cutoff=0.9898)\n",
    "#         ### Compute spectrum and histograms\n",
    "# #         dict_sample=f_compute_hist_spect(images,bins) ## list of 4 numpy arrays \n",
    "#         ### Compute chi squares\n",
    "# #         dict_chisqrs=f_compute_chisqr(dict_val,dict_sample)\n",
    "\n",
    "# #         dict1.update(dict_chisqrs)\n",
    "# #         dict1.update({'num_large':high_pixel})\n",
    "# #         dict1.update(dict_sample)\n",
    "    \n",
    "#         num_large[i]=high_pixel\n",
    "    \n",
    "    \n",
    "    \n",
    "# t1=time.time()\n",
    "# df=df_files.copy().head(50)\n",
    "# df = cudf.DataFrame.from_pandas(df)\n",
    "\n",
    "# dict1={}\n",
    "# t2=time.time()\n",
    "# df_temp=df.apply_rows(f_get_computed_dict_gpu,\n",
    "#                     incols=['fname','img_type'],\n",
    "#                     outcols={'num_large':np.float64},\n",
    "#                     kwargs={'dict_val':dict_val,'bins':bins})\n",
    "# # keys=dict1[0].keys()\n",
    "# # ### Convert list of dicts to dict of lists\n",
    "# # dict_list={key:[k[key] for k in dict1] for key in keys}\n",
    "# # ### Add columns to Dataframe\n",
    "# # for key in dict_list.keys():\n",
    "# #     df[key]=dict_list[key]\n",
    "    \n",
    "# t3=time.time()\n",
    "# print(\"Time \",t3-t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to file\n",
    "# df.to_csv(main_dir+'df_processed.csv',sep=',',index=False)\n",
    "df[['epoch','step']]=df[['epoch','step']].astype(int)\n",
    "df.to_pickle(main_dir+'df_processed.pkle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "# df_2=pd.read_csv(main_dir+'df_processed.csv',sep=',')\n",
    "df_2=pd.read_pickle(main_dir+'df_processed.pkle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
