{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from output files\n",
    "### Analyze the output from a single LBANN run\n",
    "March 9, 2020 \\\n",
    "April 6, 2020 : Major edit to store files in order of epochs \\\n",
    "April 21, 2020: Major edit, added jupyter widgets to compare pixel intensity plots \\\n",
    "May 8, 2020: Major edit, using all images for a given batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess as sp\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import time\n",
    "from scipy import fftpack\n",
    "from ipywidgets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/global/u1/v/vpa/project/jpt_notebooks/Cosmology/Cosmo_GAN/LBANN/lbann_cosmogan/3_analysis/')\n",
    "from modules_image_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4. + 1e-8) - 1.\n",
    "\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare lbann images with input and keras code images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "### Load validation input samples\n",
    "img_raw='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_2_smoothing_200k/train.npy'\n",
    "a1=np.load(img_raw)[:10000]\n",
    "s_raw=f_transform(a1[:,:,:,0])[:10000]\n",
    "\n",
    "print(s_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 128, 128)\n",
      "(5000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "### Load images from keras code\n",
    "img_keras='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/exagan1/run5/models/gen_imgs.npy'\n",
    "img_keras='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/exagan1/run6/models/gen_imgs.npy'\n",
    "\n",
    "s_keras=[[] for i in range(2)]\n",
    "for count,i in enumerate([5,6]):\n",
    "    img_keras='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/exagan1/run{0}_fixed_cosmology/models/gen_imgs.npy'.format(str(i))\n",
    "    a1=np.load(img_keras)\n",
    "    s_keras[count]=a1[:,:,:]\n",
    "\n",
    "img_keras='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/exagan1/run7_no_truncated_normal/models/gen_imgs.npy'\n",
    "a1=np.load(img_keras)\n",
    "s_keras.append(a1[:,:,:])\n",
    "print(s_keras[2].shape)\n",
    "\n",
    "img_keras='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/exagan1/run8_no_truncated_normal/models/gen_imgs.npy'\n",
    "a1=np.load(img_keras)\n",
    "s_keras.append(a1[:,:,:])\n",
    "print(s_keras[3].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/generate_images/20200806_070111_gen_img_exagan/dump_outs/trainer0/model0/sgd.testing.epoch.3.step.3_gen_img_instance1_activation_output0.npy', '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/generate_images/20200806_070111_gen_img_exagan/dump_outs/trainer0/model0/sgd.testing.epoch.0.step.0_gen_img_instance1_activation_output0.npy', '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/generate_images/20200806_070111_gen_img_exagan/dump_outs/trainer0/model0/sgd.testing.epoch.1.step.1_gen_img_instance1_activation_output0.npy', '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/generate_images/20200806_070111_gen_img_exagan/dump_outs/trainer0/model0/sgd.testing.epoch.2.step.2_gen_img_instance1_activation_output0.npy']\n",
      "(10023, 128, 128) 0.99845964\n"
     ]
    }
   ],
   "source": [
    "# ### Extract a few images generated by Lbann\n",
    "# parent_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/generate_images/20200702_074157_gen_img_070005_batchsize_256/'\n",
    "parent_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/generate_images/'\n",
    "\n",
    "# fldr='20200806_065947_gen_img_exagan/'\n",
    "\n",
    "fldr='20200806_070111_gen_img_exagan/'\n",
    "\n",
    "\n",
    "f_strg=parent_dir+fldr+'dump_outs/trainer0/model0/sgd.testing.epoch.*.step.*_gen_img_instance1_activation_output0.npy'\n",
    "f_list=glob.glob(f_strg)\n",
    "print(f_list)\n",
    "\n",
    "arr=[np.load(fname)[:,0,:,:] for fname in f_list]\n",
    "s_lbann=np.vstack(arr)\n",
    "print(s_lbann.shape,np.max(s_lbann))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_compute_chisqr(dict_val,dict_sample):\n",
    "    '''\n",
    "    Compute chi-square values for sample w.r.t input images\n",
    "    Input: 2 dictionaries with 4 keys for histogram and spectrum values and errors\n",
    "    '''\n",
    "    ### !!Both pixel histograms MUST have same bins and normalization!\n",
    "    ### Compute chi-sqr\n",
    "    ### Used in keras code : np.sum(np.divide(np.power(valhist - samphist, 2.0), valhist))\n",
    "    ###  chi_sqr :: sum((Obs-Val)^2/(Val))\n",
    "    \n",
    "    chisqr_dict={}\n",
    "    \n",
    "    val_dr=dict_val['hist_val'].copy()\n",
    "    val_dr[val_dr<=0.]=1.0    ### Avoiding division by zero for zero bins\n",
    "    \n",
    "    sq_diff=(dict_val['hist_val']-dict_sample['hist_val'])**2\n",
    "    \n",
    "    size=len(dict_val['hist_val'])\n",
    "    l1,l2=int(size*0.3),int(size*0.7)\n",
    "    keys=['chi_1a','chi_1b','chi_1c','chi_1']\n",
    "    \n",
    "    for (key,start,end) in zip(keys,[0,l1,l2,0],[l1,l2,None,None]):  # 4 lists : small, medium, large pixel values and full \n",
    "        chisqr_dict.update({key:np.sum(np.divide(sq_diff[start:end],val_dr[start:end]))})\n",
    "    \n",
    "    idx=None  # Choosing the number of histograms to use. Eg : -5 to skip last 5 bins\n",
    "#     chisqr_dict.update({'chi_sqr1':})\n",
    "    \n",
    "    chisqr_dict.update({'chi_2':np.sum(np.divide(sq_diff[:idx],1.0))}) ## chi-sqr without denominator division\n",
    "    chisqr_dict.update({'chi_imgvar':np.sum(dict_sample['hist_err'][:idx])/np.sum(dict_val['hist_err'][:idx])}) ## measures total spread in histograms wrt to input data\n",
    "    \n",
    "    idx=60\n",
    "    spec_diff=(dict_val['spec_val']-dict_sample['spec_val'])**2\n",
    "    ### computing the spectral loss chi-square\n",
    "    chisqr_dict.update({'chi_spec1':np.sum(spec_diff[:idx]/dict_sample['spec_val'][:idx]**2)})\n",
    "    \n",
    "    ### computing the spectral loss chi-square\n",
    "    chisqr_dict.update({'chi_spec2':np.sum(spec_diff[:idx]/dict_sample['spec_err'][:idx]**2)})\n",
    "    \n",
    "    return chisqr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 16920\n",
      "6 20200\n",
      "7 23680\n",
      "4 14720\n"
     ]
    }
   ],
   "source": [
    "## Get LBANN training run data\n",
    "# prefix='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200701_065330_batchsize_512/dump_outs/trainer0/model0/'\n",
    "# lst=[(58,23120),(58,23360),(3,1200),(51,20640),(49,19760),(47,18880)]\n",
    "\n",
    "# prefix='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200701_070005_batchsize_256/dump_outs/trainer0/model0/'\n",
    "# lst=[(30,24400),(29,23680),(48,38160),(42,33760),(36,29120)]\n",
    "\n",
    "# prefix='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200718_135530_batchsize_256/dump_outs/trainer0/model0/'\n",
    "# lst=[(12,10220),(21,17380),(25,20340),(24,19720)]\n",
    "\n",
    "# prefix='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200725_204329_batchsize_256/dump_outs/trainer0/model0/'\n",
    "# lst=[(17,14207),(16,13380),(18,14922),(16,13089),(18,14730)]\n",
    "\n",
    "# prefix='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200725_172458_batchsize_64/dump_outs/trainer0/model0/'\n",
    "# lst=[(5,18209),(10,34243),(4,15055),(5,18209),(7,23293)]\n",
    "\n",
    "# prefix='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200725_172458_batchsize_64/dump_outs/trainer0/model0/'\n",
    "# lst=[(5,18209),(10,34243),(4,15055),(5,18209),(7,23293)]\n",
    "\n",
    "prefix='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200805_124242_batchsize_64/dump_outs/trainer0/model0/'\n",
    "lst=[(5,16920),(6,20200),(7,23680),(4,14720)]\n",
    "s_new=[[] for i in range(len(lst))]\n",
    "for count,run in enumerate(lst):\n",
    "    epoch,step=run[0],run[1]\n",
    "    print(epoch,step)\n",
    "    fname=prefix+'sgd.training.epoch.{0}.step.{1}_gen_img_instance1_activation_output0.npy'.format(epoch,step)\n",
    "    s_new[count]=np.load(fname)[:,0,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_samples={'raw': s_raw[:3000],'keras1':s_keras[0],'keras2':s_keras[1],'keras3':s_keras[2],'keras4':s_keras[3]}\n",
    "dict_samples.update({'lbann1':s_lbann})\n",
    "for count,i in enumerate(s_new):\n",
    "    dict_samples.update({'new_'+str(count):i})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cropping out large pixel values\n",
    "\n",
    "# for key in dict_samples.keys():\n",
    "#     print(key)\n",
    "# # for key in ['new_0','new_1']:\n",
    "#     print(key,dict_samples[key].shape)\n",
    "#     dict_samples[key]=np.array([n for n in dict_samples[key] if np.max(n)<0.98])[:256] ### pixel intensity < 400\n",
    "#     print(key,dict_samples[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw 773.7213676777742\n",
      "keras1 1167.816620448088\n",
      "keras2 816.8223718776532\n",
      "keras3 982.2991666608887\n",
      "keras4 724.1099644374552\n",
      "lbann1 5189.547859704607\n",
      "new_0 955.9997683274008\n",
      "new_1 568.8696596504692\n",
      "new_2 516.0702173692437\n",
      "new_3 1008.0614009814757\n"
     ]
    }
   ],
   "source": [
    "for key in dict_samples.keys():\n",
    "    print(key,f_invtransform(np.max(dict_samples[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chi-square values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_compute_chisqr(dict_val,dict_sample):\n",
    "    '''\n",
    "    Compute chi-square values for sample w.r.t input images\n",
    "    Input: 2 dictionaries with 4 keys for histogram and spectrum values and errors\n",
    "    '''\n",
    "    ### !!Both pixel histograms MUST have same bins and normalization!\n",
    "    ### Compute chi-sqr\n",
    "    ### Used in keras code : np.sum(np.divide(np.power(valhist - samphist, 2.0), valhist))\n",
    "    ###  chi_sqr :: sum((Obs-Val)^2/(Val))\n",
    "    \n",
    "    chisqr_dict={}\n",
    "    \n",
    "    val_dr=dict_val['hist_val'].copy()\n",
    "    val_dr[val_dr<=0.]=1.0    ### Avoiding division by zero for zero bins\n",
    "    \n",
    "    sq_diff=(dict_val['hist_val']-dict_sample['hist_val'])**2\n",
    "    \n",
    "    size=len(dict_val['hist_val'])\n",
    "    l1,l2=int(size*0.3),int(size*0.7)\n",
    "    keys=['chi_1a','chi_1b','chi_1c','chi_1']\n",
    "    \n",
    "    for (key,start,end) in zip(keys,[0,l1,l2,0],[l1,l2,None,None]):  # 4 lists : small, medium, large pixel values and full \n",
    "        chisqr_dict.update({key:np.sum(np.divide(sq_diff[start:end],val_dr[start:end]))})\n",
    "    \n",
    "    idx=None  # Choosing the number of histograms to use. Eg : -5 to skip last 5 bins\n",
    "#     chisqr_dict.update({'chi_sqr1':})\n",
    "    \n",
    "    chisqr_dict.update({'chi_2':np.sum(np.divide(sq_diff[:idx],1.0))}) ## chi-sqr without denominator division\n",
    "    chisqr_dict.update({'chi_imgvar':np.sum(dict_sample['hist_err'][:idx])/np.sum(dict_val['hist_err'][:idx])}) ## measures total spread in histograms wrt to input data\n",
    "    \n",
    "    idx=60\n",
    "    spec_diff=(dict_val['spec_val']-dict_sample['spec_val'])**2\n",
    "    ### computing the spectral loss chi-square\n",
    "    chisqr_dict.update({'chi_spec1':np.sum(spec_diff[:idx]/dict_sample['spec_val'][:idx]**2)})\n",
    "    \n",
    "    ### computing the spectral loss chi-square\n",
    "    chisqr_dict.update({'chi_spec2':np.sum(spec_diff[:idx]/dict_sample['spec_err'][:idx]**2)})\n",
    "    \n",
    "    return chisqr_dict\n",
    "\n",
    "\n",
    "def f_compute_hist_spect(sample,bins):\n",
    "    ''' Compute pixel intensity histograms and radial spectrum for 2D arrays\n",
    "    Input : Image arrays and bins\n",
    "    Output: dictionary with 5 arrays : Histogram values, errors and bin centers, Spectrum values and errors.\n",
    "    '''\n",
    "    ### Compute pixel histogram for row\n",
    "    gen_hist,gen_err,hist_bins=f_batch_histogram(sample,bins=bins,norm=True,hist_range=None)\n",
    "    ### Compute spectrum for row\n",
    "    spec,spec_err=f_compute_spectrum(sample,plot=False)\n",
    "\n",
    "    dict1={'hist_val':gen_hist,'hist_err':gen_err,'hist_bin_centers':hist_bins,'spec_val':spec,'spec_err':spec_err }\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw (3000, 128, 128)\n",
      "keras1 (5000, 128, 128)\n",
      "keras2 (5000, 128, 128)\n",
      "keras3 (5000, 128, 128)\n",
      "keras4 (5000, 128, 128)\n",
      "lbann1 (10023, 128, 128)\n",
      "new_0 (64, 128, 128)\n",
      "new_1 (64, 128, 128)\n",
      "new_2 (64, 128, 128)\n",
      "new_3 (64, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "### Compute spectrum for keras and lbann image batches\n",
    "df1=pd.DataFrame([])\n",
    "\n",
    "s_input=s_raw[:]\n",
    "transform=False  # If true, it computes histogram in the orignal scale of pixels ie. 0-2000 \n",
    "bins=np.concatenate([np.array([-0.5]),np.arange(0.5,20.5,1),np.arange(20.5,100.5,5),np.arange(100.5,1000.5,50),np.array([2000])]) #bin edges to use\n",
    "# bins=np.array([-0.5,0.5,1.5,2.5,3.5,4.5,5.5,15.5,25.5,75.5,125.5,500.5]).astype(np.float64)\n",
    "\n",
    "if not transform: bins=f_transform(bins)   ### scale to (-1,1)\n",
    "# bins=100\n",
    "### Get pixel histogram and spectrum of all input data\n",
    "dict_val=f_compute_hist_spect(s_input,bins)\n",
    "del s_input\n",
    "\n",
    "for name,images in dict_samples.items():\n",
    "    print(name,images.shape)\n",
    "    ### Compute spectrum and histograms\n",
    "    dict_img=f_compute_hist_spect(images,bins) ## list of 5 numpy arrays \n",
    "    ### Compute chi squares\n",
    "    dict1=f_compute_chisqr(dict_val,dict_img)\n",
    "    dict1.update({'name':name})\n",
    "#     print(dict1)\n",
    "    df1=df1.append(dict1,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chi_1</th>\n",
       "      <th>chi_1a</th>\n",
       "      <th>chi_1b</th>\n",
       "      <th>chi_1c</th>\n",
       "      <th>chi_2</th>\n",
       "      <th>chi_imgvar</th>\n",
       "      <th>chi_spec1</th>\n",
       "      <th>chi_spec2</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.838426</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>64.630671</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>1.421143</td>\n",
       "      <td>0.038280</td>\n",
       "      <td>16780.929221</td>\n",
       "      <td>keras1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>1.477085</td>\n",
       "      <td>0.045803</td>\n",
       "      <td>20541.431537</td>\n",
       "      <td>keras2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>1.463584</td>\n",
       "      <td>0.021846</td>\n",
       "      <td>5646.684223</td>\n",
       "      <td>keras3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>1.421945</td>\n",
       "      <td>0.033614</td>\n",
       "      <td>18804.677014</td>\n",
       "      <td>keras4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.160468</td>\n",
       "      <td>0.126459</td>\n",
       "      <td>0.025951</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>1.512923</td>\n",
       "      <td>0.908326</td>\n",
       "      <td>405758.574370</td>\n",
       "      <td>lbann1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010095</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>13.806410</td>\n",
       "      <td>0.304695</td>\n",
       "      <td>1812.721447</td>\n",
       "      <td>new_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>12.018862</td>\n",
       "      <td>0.289086</td>\n",
       "      <td>1817.789213</td>\n",
       "      <td>new_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.007496</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>14.556343</td>\n",
       "      <td>0.144588</td>\n",
       "      <td>630.842789</td>\n",
       "      <td>new_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013199</td>\n",
       "      <td>0.006341</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>12.496791</td>\n",
       "      <td>0.205869</td>\n",
       "      <td>1189.958431</td>\n",
       "      <td>new_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      chi_1    chi_1a    chi_1b    chi_1c     chi_2  chi_imgvar  chi_spec1  \\\n",
       "0  0.000238  0.000025  0.000084  0.000129  0.000003    1.838426   0.000973   \n",
       "1  0.000311  0.000074  0.000039  0.000198  0.000058    1.421143   0.038280   \n",
       "2  0.000928  0.000474  0.000269  0.000185  0.000295    1.477085   0.045803   \n",
       "3  0.002907  0.002594  0.000056  0.000257  0.001918    1.463584   0.021846   \n",
       "4  0.002724  0.001654  0.000642  0.000428  0.001215    1.421945   0.033614   \n",
       "5  0.160468  0.126459  0.025951  0.008058  0.031252    1.512923   0.908326   \n",
       "6  0.010095  0.004923  0.002598  0.002574  0.000426   13.806410   0.304695   \n",
       "7  0.012598  0.002225  0.002188  0.008185  0.001568   12.018862   0.289086   \n",
       "8  0.015469  0.007496  0.004476  0.003497  0.003064   14.556343   0.144588   \n",
       "9  0.013199  0.006341  0.001892  0.004966  0.001104   12.496791   0.205869   \n",
       "\n",
       "       chi_spec2    name  \n",
       "0      64.630671     raw  \n",
       "1   16780.929221  keras1  \n",
       "2   20541.431537  keras2  \n",
       "3    5646.684223  keras3  \n",
       "4   18804.677014  keras4  \n",
       "5  405758.574370  lbann1  \n",
       "6    1812.721447   new_0  \n",
       "7    1817.789213   new_1  \n",
       "8     630.842789   new_2  \n",
       "9    1189.958431   new_3  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_pixel_intensity(s_lbann[330],label='',normalize=False,log_scale=True,mode='simple')\n",
    "# f_pixel_intensity(f_invtransform(s_lbann[330]),label='',normalize=False,log_scale=True,mode='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_widget_compare(sample_names,sample_dict,Fig_type='pixel',rescale=True,log_scale=True,bins=25,mode='avg',normalize=True,bkgnd=[]):\n",
    "    '''\n",
    "    Module to make widget plots for pixel intensity or spectrum comparison for multiple sample sets\n",
    "    '''\n",
    "    \n",
    "    ### Crop out large pixel values\n",
    "    for key in sample_names:\n",
    "        print(sample_dict[key].shape)\n",
    "        sample_dict[key]=np.array([arr for arr in sample_dict[key] if np.max(arr)<=0.994])\n",
    "        print(sample_dict[key].shape)\n",
    "    \n",
    "    img_list=[sample_dict[key] for key in sample_names]\n",
    "    label_list=list(sample_names)\n",
    "    \n",
    "    if rescale: \n",
    "        for count,img in enumerate(img_list):\n",
    "            img_list[count]=f_invtransform(img)\n",
    "        if len(bkgnd): bkgnd=f_invtransform(bkgnd)\n",
    "#         hist_range=(0,2000)\n",
    "    else:\n",
    "        bins=f_transform(bins)\n",
    "#         hist_range=(-1,0.996)\n",
    "    assert Fig_type in ['pixel','spectrum'],\"Invalid mode %s\"%(mode)\n",
    "    \n",
    "    if Fig_type=='pixel':\n",
    "#         f_compare_pixel_intensity(img_lst=img_list,label_lst=label_list,normalize=normalize,log_scale=log_scale, mode=mode,bins=bins,hist_range=hist_range)\n",
    "        f_compare_pixel_intensity(img_lst=img_list,label_lst=label_list,normalize=normalize,log_scale=log_scale, mode=mode,bins=bins,hist_range=None,bkgnd_arr=bkgnd)\n",
    "\n",
    "    elif Fig_type=='spectrum':\n",
    "        f_compare_spectrum(img_lst=img_list,label_lst=label_list,log_scale=log_scale,bkgnd_arr=bkgnd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=np.array([-0.5,0.5,1.5,2.5,3.5,4.5,5.5,15.5,25.5,75.5,125.5,500.5]).astype(np.float64)\n",
    "bins=np.concatenate([np.array([-0.5]),np.arange(0.5,20.5,1),np.arange(20.5,100.5,5),np.arange(100.5,1000.5,50),np.array([2000])]) #bin edges to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b595772c37d547ed8920b4430f7518de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='sample_names', options=('raw', 'keras1', 'keras2', 'keras3',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f_widget_compare(sample_names, sample_dict, Fig_type='pixel', rescale=True, log_scale=True, bins=25, mode='avg', normalize=True, bkgnd=[])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict_samples={'raw': s_raw,'keras1':s_keras[0],'keras2':s_keras[1],'s_new1':s_new1,'s_new2':s_new2,'s_new3':s_new3}\n",
    "\n",
    "bkgnd=s_raw\n",
    "# bkgnd=[]\n",
    "interact_manual(f_widget_compare,sample_dict=fixed(dict_samples),\n",
    "                sample_names=SelectMultiple(options=dict_samples.keys()),\n",
    "                Fig_type=ToggleButtons(options=['pixel','spectrum']),\n",
    "                bins=fixed(bins),\n",
    "                mode=['avg','simple'],bkgnd=fixed(bkgnd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_plot_grid(s_keras[0][:18],cols=6,fig_size=(10,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7586e350f9d4a6d8054d6b1eca26c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_plot_grid(s_new[0][18:36],cols=6,fig_size=(10,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 951.1016419024327\n",
      "58 865.6115300727208\n",
      "166 974.3120460778867\n"
     ]
    }
   ],
   "source": [
    "for count,i in enumerate(s_new[0]):\n",
    "    if np.max(i)>0.9898:\n",
    "        print(count,f_invtransform(np.max(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
