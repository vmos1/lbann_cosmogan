{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from output files\n",
    "### Analyze the output from a single LBANN run\n",
    "March 9, 2020 \\\n",
    "April 6, 2020 : Major edit to store files in order of epochs \\\n",
    "April 21, 2020: Major edit, added jupyter widgets to compare pixel intensity plots\n",
    "May 8, 2020: Major edit, using all images for a given batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess as sp\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import time\n",
    "from scipy import fftpack\n",
    "# from ipywidgets import interact, interact_manual,fixed, SelectMultiple, IntText, IntSlider, FloatSlider,SelectionSlider,BoundedIntText\n",
    "from ipywidgets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook modules_image_analysis.ipynb to script\n",
      "[NbConvertApp] Writing 14928 bytes to modules_image_analysis.py\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/global/u1/v/vpa/project/jpt_notebooks/Cosmology/Cosmo_GAN/LBANN/lbann_cosmogan/3_analysis/')\n",
    "from modules_image_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4. + 1e-8) - 1.\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules for Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_files_df_sorted():\n",
    "    '''\n",
    "    Module to create Dataframe with filenames for each epoch and step\n",
    "    Sorts by step and epoch\n",
    "    '''\n",
    "    \n",
    "    ## Get images files and .npy arrays for each image in dump_outs folder\n",
    "    t1=time.time()\n",
    "    files_dict={}\n",
    "    keys=['train_gen','train_input','val_gen','val_input']\n",
    "    file_strg_lst=['model0-training*-gen_img*-output0.npy','model0-training*-inp_img*-output0.npy','model0-validation*-gen_img*-output0.npy','model0-validation*-inp_img*-output0.npy']\n",
    "    for key,file_strg in zip(keys,file_strg_lst):\n",
    "        files_dict[key]=np.array(glob.glob(main_dir+file_strg))\n",
    "        if files_dict[key].shape[0]>1000 : \n",
    "            print('Warning the number of files is very large. Possibility of memory overload')\n",
    "\n",
    "    df_files=pd.DataFrame([])\n",
    "    dict1={}\n",
    "    t1=time.time()\n",
    "    ### First get sorted Dataframe with file names\n",
    "    for key in keys: \n",
    "        files_arr=files_dict[key]  # Get array of files\n",
    "        print(key,len(files_arr))\n",
    "        for fname in files_arr:\n",
    "            ### Extract the Epoch number and step number from the file name\n",
    "            dict1['img_type']=key\n",
    "            dict1['epoch']=np.int32(fname.split('epoch')[-1].split('-')[0])\n",
    "            dict1['step']=np.int64(fname.split('step')[-1].split('-')[0])\n",
    "            dict1['fname']=fname\n",
    "\n",
    "            df_files=df_files.append(dict1,ignore_index=True)\n",
    "    ## Sort values\n",
    "    df_files=df_files.sort_values(by=['img_type','epoch','step']).reset_index(drop=True)\n",
    "    # df_files\n",
    "    t2=time.time()\n",
    "    print(\"Time for Sorting\",t2-t1)\n",
    "    \n",
    "    return df_files\n",
    "\n",
    "\n",
    "def f_filter_epoch(df_input,num_sliced=1):\n",
    "    '''\n",
    "    Get just the last few stored step images for each epoch\n",
    "    '''\n",
    "    print('Extracting last %s steps of each epoch'%(num_sliced))\n",
    "    df_output=pd.DataFrame([])\n",
    "    for key in ['train_gen','train_input','val_gen','val_input']: \n",
    "        ### For each type of images, get list of epochs\n",
    "        df1=df_input[df_input.img_type==key]\n",
    "        epochs=np.unique(df1.epoch.values).astype(int)\n",
    "\n",
    "        for epoch in epochs:### Extract the last few steps in each epoch\n",
    "            df2=df1[df1.epoch==epoch]\n",
    "            df_output=df_output.append(df2.iloc[-num_sliced:])  \n",
    "    \n",
    "    return df_output.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def f_get_images_df(df_files):\n",
    "    '''\n",
    "    Read dataframe with file names, read files and create new dataframe with images as numpy arrays\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def f_row(df_row):\n",
    "        '''\n",
    "        Extract image\n",
    "        '''\n",
    "        fname,key=df_row.fname,df_row.img_type\n",
    "        a1=np.load(fname)\n",
    "        if key.endswith('input'): \n",
    "            size=np.int(np.sqrt(a1.shape[-1])) ### Extract size of images (=128)\n",
    "            batch_size=a1.shape[0] ### Number of batches\n",
    "            samples=a1.reshape(batch_size,size,size)\n",
    "        elif key.endswith('gen') : samples=a1[:,0,:,:]\n",
    "        else : raise SystemError\n",
    "\n",
    "        return samples\n",
    "    \n",
    "    t1=time.time()\n",
    "    ##### Create new Dataframe with sorted images\n",
    "    df=df_files.copy()\n",
    "    df['images']=df.apply(lambda row: f_row(row), axis=1)\n",
    "    t2=time.time()\n",
    "    \n",
    "    print(\"Time for Reading images\",t2-t1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def f_get_samples(df,img_type,start_epoch=0,end_epoch=None):\n",
    "    '''\n",
    "    Module to extract images for a range of epochs given a dataframe\n",
    "    '''\n",
    "    if end_epoch==None: end_epoch=start_epoch+1\n",
    "    \n",
    "    arr=df[(df.epoch>=start_epoch) & (df.epoch<=end_epoch) & (df.img_type==img_type)].images.values\n",
    "    arr=np.vstack(arr)\n",
    "#     print(arr.shape,np.max(arr))    \n",
    "    \n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200513_121910_peters_dataset/dump_outs/\n"
     ]
    }
   ],
   "source": [
    "fldr_name='20200413_095840_exagan'\n",
    "fldr_name='20200423_122631_exagan_modified_paddding'\n",
    "fldr_name='20200424_083456_exagan_modified_padding_2'\n",
    "fldr_name='20200506_121613_exagan_200k_samples'\n",
    "fldr_name='20200513_121910_peters_dataset'\n",
    "\n",
    "\n",
    "main_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/{0}/dump_outs/'.format(fldr_name)\n",
    "print(main_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_gen 902\n",
      "train_input 902\n",
      "val_gen 226\n",
      "val_input 226\n",
      "Time for Sorting 6.468547582626343\n",
      "Extracting last 1 steps of each epoch\n",
      "Time for Reading images 11.022383213043213\n",
      "(240, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Get dataframe with file names, sorted by epoch and step\n",
    "df_files=f_get_files_df_sorted()\n",
    "\n",
    "### Slice out rows to keep only the last few steps for each epoch.\n",
    "df_files=f_filter_epoch(df_files,num_sliced=1)\n",
    "\n",
    "#############################################################\n",
    "### Read images one by one into a numpy array and create a new DataFrame\n",
    "df=f_get_images_df(df_files)\n",
    "print(df.shape)\n",
    "\n",
    "# ### Filter to keep just one step per epoch\n",
    "# df=f_filter_epoch(df,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fname</th>\n",
       "      <th>img_type</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>1230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>2460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>3690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>4920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>6150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>7380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>8610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>9840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>11070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>/global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...</td>\n",
       "      <td>train_gen</td>\n",
       "      <td>12300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch                                              fname   img_type  \\\n",
       "0    0.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "1    1.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "2    2.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "3    3.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "4    4.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "5    5.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "6    6.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "7    7.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "8    8.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "9    9.0  /global/cfs/cdirs/m3363/vayyar/cosmogan_data/r...  train_gen   \n",
       "\n",
       "      step  \n",
       "0   1230.0  \n",
       "1   2460.0  \n",
       "2   3690.0  \n",
       "3   4920.0  \n",
       "4   6150.0  \n",
       "5   7380.0  \n",
       "6   8610.0  \n",
       "7   9840.0  \n",
       "8  11070.0  \n",
       "9  12300.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some epoch-steps have different batch size\n",
    "for i,j in df.iterrows():\n",
    "    if j.images.shape[0]!=128:\n",
    "        print(i,j.epoch,j.step,j.img_type,j.images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=df[(df.img_type=='train_gen')].images.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the region without very high pixel values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs with no high pixel images [ 0  1  2  3  4  6  7  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26\n",
      " 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51\n",
      " 52 53 54 55 56 57 58 59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/v/vpa/.conda/envs/v_py3/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551e7adc83d24a4b96294ad2542cd740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aab22dcd240>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the maximum of each images for an epoch, count number of these above the cutoff\n",
    "cutoff=0.9966\n",
    "num_large=np.zeros(samples.shape)\n",
    "\n",
    "for count,ee in enumerate(samples):\n",
    "    max_arr=np.amax(ee,axis=(1,2)) # maximum of each image\n",
    "    num_large[count]=max_arr[max_arr>cutoff].shape[0]\n",
    "\n",
    "### Epochs with no high pixel values\n",
    "zero_idx=np.where(num_large==0.0)[0]\n",
    "other_idx=np.where(num_large>0.0)[0]\n",
    "\n",
    "print(\"Epochs with no high pixel images\",zero_idx)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(zero_idx,num_large[zero_idx],linestyle='',marker='*',color='r')\n",
    "plt.plot(other_idx,num_large[other_idx],linestyle='',marker='D',color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/v/vpa/.conda/envs/v_py3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05248a4100e4886899fbe577aee1dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Plot the maximum across samples for each epoch\n",
    "\n",
    "def f_plot_max_values(samples,cutoff=0.994):\n",
    "    '''\n",
    "    Make a plot of max values of images of a given set of sample images\n",
    "    cutoff used to discard high values\n",
    "    '''\n",
    "    ### Get max pixel values of images\n",
    "    max_values=np.array([np.max(i) for i in samples])\n",
    "    ### Less than cutoff\n",
    "    lesser_idx=np.where(max_values<cutoff)[0]\n",
    "    higher_idx=np.where(max_values>=cutoff)[0]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(lesser_idx,max_values[lesser_idx],linestyle='',marker='*',color='r')\n",
    "    plt.plot(higher_idx,max_values[higher_idx],linestyle='',marker='D',color='b')\n",
    "\n",
    "    plt.axhline(y=cutoff,linestyle='--',color='k')\n",
    "    plt.ylim(0.9,1.0)\n",
    "    \n",
    "f_plot_max_values(samples,0.9966)  ### The full dataset has a max value of just over 0.9966\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# samples=f_invtransform(f_get_samples(df,'val_gen',epoch,epoch+1))\n",
    "# ### Recomputing validation histogram to match bins with generated image\n",
    "# samples_input=f_invtransform(f_get_samples(df,'train_input',0,60))\n",
    "                       \n",
    "# samples.shape,samples_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "### Computing the chi-sqr, including high pixel values. \n",
    "# This takes time as the histogram is determined for each epoch\n",
    "\n",
    "num_epochs=60\n",
    "chi_sqr=np.zeros(num_epochs)\n",
    "chi_sqr2=np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "    print(epoch)\n",
    "    samples=f_invtransform(f_get_samples(df,'val_gen',epoch,epoch+1))\n",
    "    ### Recomputing validation histogram to match bins with generated image\n",
    "    samples_input=f_invtransform(f_get_samples(df,'train_input',0,60))\n",
    "    max_val=np.max([np.max(samples),np.max(samples_input)])\n",
    "    val_hist,val_err=f_pixel_intensity(samples_input,plot=False,normalize=False,bins=100,hist_range=(0,max_val))\n",
    "    val_dr=val_hist.copy()\n",
    "    val_dr[val_dr<=0.]=1.0    ### Avoiding division by zero for zero bins\n",
    "    \n",
    "    gen_hist,gen_err=f_pixel_intensity(samples,plot=False,normalize=False,bins=100,hist_range=(0,max_val))\n",
    "    sq_diff=(gen_hist-val_hist)**2\n",
    "    chi_sqr[epoch]=np.sum(np.divide(sq_diff,val_dr)) ###  sum((Obs-Val)^2/(Val))\n",
    "    chi_sqr2[epoch]=np.sum(sq_diff-val_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computing the chi-sqr, ignoring high pixel values\n",
    "# the histogram bin is determined by the input maximum value\n",
    "\n",
    "num_epochs=60\n",
    "chi_sqr=np.zeros(num_epochs)\n",
    "chi_sqr2=np.zeros(num_epochs)\n",
    "\n",
    "samples_input=f_invtransform(f_get_samples(df,'train_input',0,60))\n",
    "max_val=np.max(samples_input)\n",
    "val_hist,val_err=f_pixel_intensity(samples_input,plot=False,normalize=False,bins=100,hist_range=(0,max_val))\n",
    "val_dr=val_hist.copy()\n",
    "val_dr[val_dr<=0.]=1.0    ### Avoiding division by zero for zero bins\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "    samples=f_invtransform(f_get_samples(df,'val_gen',epoch,epoch+1))\n",
    "    ### Recomputing validation histogram to match bins with generated image\n",
    "    gen_hist,gen_err=f_pixel_intensity(samples,plot=False,normalize=False,bins=100,hist_range=(0,max_val))\n",
    "    sq_diff=(gen_hist-val_hist)**2\n",
    "    chi_sqr[epoch]=np.sum(np.divide(sq_diff,val_dr)) ###  sum((Obs-Val)^2/(Val))\n",
    "    chi_sqr2[epoch]=np.sum(sq_diff-val_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905db5b7518b43fd899ae2683da2d745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aaade1ad7b8>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(chi_sqr,marker='o')\n",
    "# plt.plot(chi_sqr2,marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67e46b919124eb3aaf85250aa575f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aaade34ada0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(chi_sqr,marker='o')\n",
    "# plt.plot(chi_sqr2,marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore image samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_pixel_intensity(samples2,'s2',normalize=True,mode='simple',bins=50)\n",
    "# f_compare_pixel_intensity([samples2[20:60],samples4,['s2','s4'],normalize=normalize,log_scale=log_scale, mode=mode,bins=bins)\n",
    "# f_compute_spectrum(samples2)\n",
    "# f_compare_spectrum([samples2[20:60],samples4],['s2','s4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_widget_individual(df,img_type='val_gen',idx_range=(0,50),Fig_type='pixel',normalize=True,log_scale=True,rescale=True,mode='avg'):\n",
    "    '''\n",
    "    Module to plot pixel intensity or power spectrum for a given sample set of images\n",
    "    Options for normalization, log-scal, and rescale\n",
    "    Rescale converts image pixel values from (-1,1) to the original pixel range\n",
    "    2 Fig_type: pixel-> pixel intensity and spectrum -> power spectrum\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    start,end=idx_range[0],idx_range[1]\n",
    "    print('Index Range %s - %s'%(start,end))\n",
    "    try :\n",
    "        sliced_arr=f_get_samples(df,img_type=img_type,start_epoch=start,end_epoch=end)\n",
    "        if sliced_arr.shape[0]<1:\n",
    "            print('Input indices %s %s are invalid.\\nUsing full array'%(start,end))\n",
    "            start0,end=0,'end'\n",
    "            sliced_arr=f_get_samples(df,img_type=img_type)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if rescale: ### Converting from pixel intensity range (-1,1) to original range\n",
    "        sliced_arr=f_invtransform(sliced_arr)\n",
    "    print('Array size used',sliced_arr.shape)\n",
    "    \n",
    "    if Fig_type=='pixel':\n",
    "        f_pixel_intensity(sliced_arr,label=img_type+': {0}-{1}'.format(str(start),str(end)),normalize=normalize,log_scale=log_scale,mode=mode)\n",
    "    elif Fig_type=='spectrum':\n",
    "        f_compute_spectrum(sliced_arr,label=img_type+': {0}-{1}'.format(str(start),str(end)),log_scale=log_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1488b5f4765a4c2985f0afa88756c687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntRangeSlider(value=(0, 60), description='idx_range', max=80), ToggleButtons(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f_widget_individual(df, img_type='val_gen', idx_range=(0, 50), Fig_type='pixel', normalize=True, log_scale=True, rescale=True, mode='avg')>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(f_widget_individual,df=fixed(df),img_type=fixed('val_gen'),\n",
    "                Fig_type=ToggleButtons(options=['pixel','spectrum']),mode=['avg','simple'],\n",
    "                idx_range=IntRangeSlider(value=(0,60),min=0,max=80,step=1),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_widget_compare(sample_names,sample_dict,Fig_type='pixel',rescale=True,log_scale=True,bins=25,mode='avg',normalize=True):\n",
    "    '''\n",
    "    Module to make widget plots for pixel intensity or spectrum comparison for multiple sample sets\n",
    "    '''\n",
    "    img_list=[sample_dict[key] for key in sample_names]\n",
    "    label_list=list(sample_names)\n",
    "    \n",
    "    if rescale: \n",
    "        for count,img in enumerate(img_list):\n",
    "            img_list[count]=f_invtransform(img)\n",
    "    \n",
    "    assert Fig_type in ['pixel','spectrum'],\"Invalid mode %s\"%(mode)\n",
    "    \n",
    "    if Fig_type=='pixel':\n",
    "        f_compare_pixel_intensity(img_lst=img_list,label_lst=label_list,normalize=normalize,log_scale=log_scale, mode=mode,bins=bins)\n",
    "    elif Fig_type=='spectrum':\n",
    "        f_compare_spectrum(img_lst=img_list,label_lst=label_list,log_scale=log_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe1cf14bc4c4c669d27c80296e71f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='sample_names', options=('0:4', '17:20', '25:27', '34:37', '4…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f_widget_compare(sample_names, sample_dict, Fig_type='pixel', rescale=True, log_scale=True, bins=25, mode='avg', normalize=True)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img_list,labels_list=f_get_sample_epochs(df,'val_gen',10)\n",
    "\n",
    "img_list,labels_list=[],[]\n",
    "for epoch_range in [(0,4),(17,20),(25,27),(34,37),(44,51),(53,59)]:\n",
    "    start,end=epoch_range[0],epoch_range[1]\n",
    "    img_list.append(f_get_samples(df,'val_gen',start,end))\n",
    "    labels_list.append('%s:%s'%(str(start),str(end)))\n",
    "\n",
    "dict_samples=dict.fromkeys(labels_list)\n",
    "for key,val in zip(labels_list,img_list): dict_samples[key]=val\n",
    "\n",
    "### Compare with input\n",
    "dict_samples['val input']=f_get_samples(df,img_type='val_input')\n",
    "interact_manual(f_widget_compare,sample_dict=fixed(dict_samples),\n",
    "                sample_names=SelectMultiple(options=dict_samples.keys()),\n",
    "                Fig_type=ToggleButtons(options=['pixel','spectrum']),bins=IntText(value=50),mode=['avg','simple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare image types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99780107\n"
     ]
    }
   ],
   "source": [
    "### Available options : keys=['train_gen','train_input','val_gen','val_input']\n",
    "start,end=35,46\n",
    "samples1=f_get_samples(df,'val_gen',start,end)\n",
    "samples2=f_get_samples(df,'val_input',0,60)\n",
    "samples3=f_get_samples(df,'train_gen',start,end)\n",
    "samples4=f_get_samples(df,'train_input')\n",
    "\n",
    "print(np.max(samples1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43201ef93733466aa50c637d569c7f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='sample_names', options=('s1', 's2', 's3', 's4'), value=()), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f_widget_compare(sample_names, sample_dict, Fig_type='pixel', rescale=True, log_scale=True, bins=25, mode='avg', normalize=True)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_samples={'s1':samples1, 's2': samples2, 's3': samples3, 's4':samples4}\n",
    "interact_manual(f_widget_compare,sample_dict=fixed(dict_samples),\n",
    "                sample_names=SelectMultiple(options=dict_samples.keys()),\n",
    "                Fig_type=ToggleButtons(options=['pixel','spectrum']),bins=IntText(value=50),mode=['avg','simple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different sample sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare lbann images with input and keras code images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 128, 128) (3000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "### Load images from keras code\n",
    "# img_keras='/global/cfs/cdirs/dasrepo/vpa/cosmogan/data/computed_data/exagan1/run_100k_samples_35epochs/models/gen_imgs.npy'\n",
    "# img_keras='/global/cfs/cdirs/dasrepo/vpa/cosmogan/data/computed_data/exagan1/run_200k_samples_24epochs/models/gen_imgs.npy'\n",
    "img_keras='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/exagan1/run_200k_samples_peter_dataset_20_epochs/models/gen_imgs.npy'\n",
    "\n",
    "a1=np.load(img_keras)\n",
    "s_keras=a1[:,:,:]\n",
    "\n",
    "### Load validation samples\n",
    "img_raw='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/very_large_dataset_val.npy'\n",
    "a1=np.load(img_raw)\n",
    "s_raw=f_transform(a1[:,:,:,0])[:3000]\n",
    "\n",
    "print(s_raw.shape,s_keras.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(1024, 128, 128) 0.9998577\n"
     ]
    }
   ],
   "source": [
    "### Extract a few images generated by Lban directly for a set of epochs\n",
    "# parent_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200506_121613_exagan_200k_samples/'\n",
    "parent_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/20200513_121910_peters_dataset/'\n",
    "\n",
    "ff=[]\n",
    "\n",
    "for epoch in [34,37]:\n",
    "    f_strg=parent_dir+'dump_outs/model0-validation-epoch{}-*gen_img*.npy'.format(epoch)\n",
    "    lst=glob.glob(f_strg)\n",
    "    ff.append(lst)\n",
    "f_list=[fle for a in ff for fle in a] ## Flattening out a list of lists\n",
    "print(len(f_list))\n",
    "\n",
    "arr=[np.load(fname)[:,0,:,:] for fname in f_list]\n",
    "s_lbann=np.vstack(arr)\n",
    "print(s_lbann.shape,np.max(s_lbann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01ee88d80ba49888c66a0e621e10877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='sample_names', options=('lbann', 'keras', 'raw'), value=()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f_widget_compare(sample_names, sample_dict, Fig_type='pixel', rescale=True, log_scale=True, bins=25, mode='avg', normalize=True)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_samples={'lbann':s_lbann, 'keras':s_keras,'raw': s_raw}\n",
    "\n",
    "interact_manual(f_widget_compare,sample_dict=fixed(dict_samples),\n",
    "                sample_names=SelectMultiple(options=dict_samples.keys()),\n",
    "                Fig_type=ToggleButtons(options=['pixel','spectrum']),\n",
    "                bins=SelectionSlider(options=np.arange(10,200,10),value=50),\n",
    "                mode=['avg','simple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
