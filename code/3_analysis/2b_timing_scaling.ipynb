{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from output files\n",
    "\n",
    "### Code to extract timing information from output files of Lbann code\n",
    "March 9, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess as sp\n",
    "import os\n",
    "import glob\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract training times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_output(fname):\n",
    "    ''' Use grep to get run times from output files\n",
    "    '''\n",
    "    cmd='grep \"run time\" {0}| grep \"training\"'.format(fname)\n",
    "    op1=sp.check_output(cmd,shell=True).decode('utf-8').split('\\n')[:-1]\n",
    "    # print(op1)\n",
    "\n",
    "    cmd='grep \"run time\" {0}| grep \"validation\"'.format(fname)\n",
    "    op2=sp.check_output(cmd,shell=True).decode('utf-8').split('\\n')[:-1]\n",
    "    \n",
    "    return op1,op2\n",
    "\n",
    "def f_get_run_times(op_arr):\n",
    "    '''\n",
    "    Get run times from output file out.log\n",
    "    '''\n",
    "    run_times=np.array([np.float(i.split(':')[-1][:-1]) for i in op_arr])\n",
    "    return run_times\n",
    "\n",
    "\n",
    "def f_store_run_times(fname):\n",
    "    \n",
    "    ### Get output \n",
    "    op1,op2=f_get_output(fname+'/out.log')\n",
    "    ### Get arrays from outputs for training and validation\n",
    "    arr1=f_get_run_times(op1)\n",
    "    arr2=f_get_run_times(op2)\n",
    "    ### Print times\n",
    "    size=len(arr1) ### Number of epochs\n",
    "    ### Compute mean and errors of times\n",
    "    train_mean,train_err=np.mean(arr1),np.std(arr1)/np.sqrt(size)\n",
    "    val_mean,val_err=np.mean(arr2),np.std(arr2)/np.sqrt(size)\n",
    "    \n",
    "    \n",
    "    ### Extract processor info from the file name\n",
    "    try:\n",
    "#         ## For slurm files\n",
    "#         lst=fname.split('/')[-1].split('.')[0].split('_')\n",
    "# #         print(lst)\n",
    "#         nodes,procs=int(lst[1]),int(lst[2].split('-')[0])\n",
    "    \n",
    "        # For out.log files\n",
    "        lst=fname.split('_')\n",
    "#         print(lst)\n",
    "        batch,nodes,procs=int(lst[4].split('bsize')[-1]),int(lst[6]),int(lst[7])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e,fle)\n",
    "        nodes,procs,cpus=None,None,None\n",
    "        pass\n",
    "    \n",
    "    job_strg='%s_%s_%s'%(nodes,procs,batch)\n",
    "    \n",
    "    keys=['train_arr','val_arr','train_mean','train_err','val_mean','val_err','num_epochs','nodes','GPUs_per_node','batchsize','job_strg']\n",
    "    values=[arr1,arr2,train_mean,train_err,val_mean,val_err,size,nodes,procs,batch,job_strg]\n",
    "    info_dict=dict(zip(keys,values))\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname=main_dir+'20200309_161923_exagan/out.log'\n",
    "# f_store_run_times(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Code for directly using slurm files\n",
    "# main_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_data/'\n",
    "# df=pd.DataFrame([])\n",
    "\n",
    "# for fle in glob.glob(main_dir+'slurm-scaling_*_*.out'):\n",
    "# #     print(fle)\n",
    "#     info_dict=f_store_run_times(fle)\n",
    "#     df=df.append(info_dict,ignore_index=True)\n",
    "    \n",
    "# df=df.sort_values(by=['nodes','GPUs_per_node']).reset_index(drop=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPUs_per_node</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>job_strg</th>\n",
       "      <th>nodes</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>train_arr</th>\n",
       "      <th>train_err</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>val_arr</th>\n",
       "      <th>val_err</th>\n",
       "      <th>val_mean</th>\n",
       "      <th>num_GPUs</th>\n",
       "      <th>batch_samples_per_GPU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1_1_32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[359.143, 354.098, 354.01, 355.164, 357.03, 35...</td>\n",
       "      <td>0.551521</td>\n",
       "      <td>356.19520</td>\n",
       "      <td>[15.6804, 15.4904, 15.1138, 16.7692, 15.5531, ...</td>\n",
       "      <td>0.163812</td>\n",
       "      <td>15.551300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1_4_128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[109.946, 106.336, 104.856, 105.27, 105.423, 1...</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>105.96690</td>\n",
       "      <td>[4.88709, 4.62119, 4.63423, 4.72335, 4.72887, ...</td>\n",
       "      <td>0.058385</td>\n",
       "      <td>4.676315</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1_8_64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[79.5875, 76.1539, 76.2804, 78.7019, 76.6649, ...</td>\n",
       "      <td>0.444776</td>\n",
       "      <td>77.87435</td>\n",
       "      <td>[2.82619, 2.97278, 3.79764, 2.86661, 2.96543, ...</td>\n",
       "      <td>0.216331</td>\n",
       "      <td>3.410702</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1_8_256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[59.2155, 54.1355, 53.9806, 55.1209, 54.3788, ...</td>\n",
       "      <td>0.497785</td>\n",
       "      <td>54.84403</td>\n",
       "      <td>[2.85722, 2.41639, 2.46879, 2.47476, 2.77964, ...</td>\n",
       "      <td>0.046266</td>\n",
       "      <td>2.536589</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1_8_32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[117.838, 111.049, 113.513, 115.824, 114.904, ...</td>\n",
       "      <td>0.628472</td>\n",
       "      <td>114.04490</td>\n",
       "      <td>[4.4348, 4.46372, 4.15406, 4.15757, 4.64013, 4...</td>\n",
       "      <td>0.255027</td>\n",
       "      <td>4.682148</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1_8_1024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[52.2489, 44.9361, 46.0965, 43.5757, 43.7856, ...</td>\n",
       "      <td>0.818162</td>\n",
       "      <td>45.10415</td>\n",
       "      <td>[3.96834, 1.93799, 1.9347, 1.94431, 3.38007, 1...</td>\n",
       "      <td>0.223550</td>\n",
       "      <td>2.288528</td>\n",
       "      <td>8.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1_8_512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[51.7851, 47.3257, 46.464, 45.7134, 46.7922, 4...</td>\n",
       "      <td>0.557007</td>\n",
       "      <td>47.06034</td>\n",
       "      <td>[3.64415, 1.96741, 2.11442, 1.95557, 2.3897, 1...</td>\n",
       "      <td>0.162875</td>\n",
       "      <td>2.256876</td>\n",
       "      <td>8.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1_8_2048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[63.1145, 41.8487, 44.9689, 41.5233, 45.2813, ...</td>\n",
       "      <td>1.889516</td>\n",
       "      <td>45.77109</td>\n",
       "      <td>[6.69585, 2.00216, 1.98863, 1.98551, 1.98065, ...</td>\n",
       "      <td>0.508531</td>\n",
       "      <td>2.747454</td>\n",
       "      <td>8.0</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2_2_128</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[106.761, 103.506, 104.514, 105.436, 103.888, ...</td>\n",
       "      <td>0.307315</td>\n",
       "      <td>104.55030</td>\n",
       "      <td>[4.66266, 4.32728, 4.29564, 4.07905, 3.99203, ...</td>\n",
       "      <td>0.099529</td>\n",
       "      <td>4.443819</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>2_4_256</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[55.2396, 53.0037, 53.7125, 54.0469, 51.1769, ...</td>\n",
       "      <td>0.408644</td>\n",
       "      <td>52.65502</td>\n",
       "      <td>[2.36805, 2.05706, 2.05694, 2.06412, 2.40571, ...</td>\n",
       "      <td>0.189731</td>\n",
       "      <td>2.471418</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2_8_512</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[29.1871, 24.4445, 25.1934, 25.033, 27.0807, 2...</td>\n",
       "      <td>0.623332</td>\n",
       "      <td>25.85925</td>\n",
       "      <td>[1.54897, 1.01397, 1.16743, 1.01751, 1.27299, ...</td>\n",
       "      <td>0.076728</td>\n",
       "      <td>1.203748</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>3_8_512</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[32.7532, 23.0088, 23.7049, 21.9365, 25.0314, ...</td>\n",
       "      <td>1.101816</td>\n",
       "      <td>25.08310</td>\n",
       "      <td>[1.14799, 0.786602, 10.0463, 0.788984, 1.9944,...</td>\n",
       "      <td>0.861504</td>\n",
       "      <td>2.002010</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4_2_256</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[65.2856, 58.7143, 57.1587, 60.0412, 57.8585, ...</td>\n",
       "      <td>0.690984</td>\n",
       "      <td>59.80198</td>\n",
       "      <td>[2.57335, 4.40686, 2.19681, 3.26529, 2.54339, ...</td>\n",
       "      <td>0.192145</td>\n",
       "      <td>2.783686</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>4_4_512</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[29.1961, 25.3785, 24.8327, 24.9893, 24.7798, ...</td>\n",
       "      <td>0.412116</td>\n",
       "      <td>25.42578</td>\n",
       "      <td>[1.45493, 1.00146, 1.04083, 1.00373, 1.03733, ...</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>1.088257</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    GPUs_per_node  batchsize  job_strg  nodes  num_epochs  \\\n",
       "0             1.0       32.0    1_1_32    1.0        10.0   \n",
       "1             4.0      128.0   1_4_128    1.0        10.0   \n",
       "2             8.0       64.0    1_8_64    1.0        10.0   \n",
       "3             8.0      256.0   1_8_256    1.0        10.0   \n",
       "4             8.0       32.0    1_8_32    1.0        10.0   \n",
       "5             8.0     1024.0  1_8_1024    1.0        10.0   \n",
       "6             8.0      512.0   1_8_512    1.0        10.0   \n",
       "7             8.0     2048.0  1_8_2048    1.0        10.0   \n",
       "8             2.0      128.0   2_2_128    2.0        10.0   \n",
       "9             4.0      256.0   2_4_256    2.0        10.0   \n",
       "10            8.0      512.0   2_8_512    2.0        10.0   \n",
       "11            8.0      512.0   3_8_512    3.0        10.0   \n",
       "12            2.0      256.0   4_2_256    4.0        10.0   \n",
       "13            4.0      512.0   4_4_512    4.0        10.0   \n",
       "\n",
       "                                            train_arr  train_err  train_mean  \\\n",
       "0   [359.143, 354.098, 354.01, 355.164, 357.03, 35...   0.551521   356.19520   \n",
       "1   [109.946, 106.336, 104.856, 105.27, 105.423, 1...   0.444558   105.96690   \n",
       "2   [79.5875, 76.1539, 76.2804, 78.7019, 76.6649, ...   0.444776    77.87435   \n",
       "3   [59.2155, 54.1355, 53.9806, 55.1209, 54.3788, ...   0.497785    54.84403   \n",
       "4   [117.838, 111.049, 113.513, 115.824, 114.904, ...   0.628472   114.04490   \n",
       "5   [52.2489, 44.9361, 46.0965, 43.5757, 43.7856, ...   0.818162    45.10415   \n",
       "6   [51.7851, 47.3257, 46.464, 45.7134, 46.7922, 4...   0.557007    47.06034   \n",
       "7   [63.1145, 41.8487, 44.9689, 41.5233, 45.2813, ...   1.889516    45.77109   \n",
       "8   [106.761, 103.506, 104.514, 105.436, 103.888, ...   0.307315   104.55030   \n",
       "9   [55.2396, 53.0037, 53.7125, 54.0469, 51.1769, ...   0.408644    52.65502   \n",
       "10  [29.1871, 24.4445, 25.1934, 25.033, 27.0807, 2...   0.623332    25.85925   \n",
       "11  [32.7532, 23.0088, 23.7049, 21.9365, 25.0314, ...   1.101816    25.08310   \n",
       "12  [65.2856, 58.7143, 57.1587, 60.0412, 57.8585, ...   0.690984    59.80198   \n",
       "13  [29.1961, 25.3785, 24.8327, 24.9893, 24.7798, ...   0.412116    25.42578   \n",
       "\n",
       "                                              val_arr   val_err   val_mean  \\\n",
       "0   [15.6804, 15.4904, 15.1138, 16.7692, 15.5531, ...  0.163812  15.551300   \n",
       "1   [4.88709, 4.62119, 4.63423, 4.72335, 4.72887, ...  0.058385   4.676315   \n",
       "2   [2.82619, 2.97278, 3.79764, 2.86661, 2.96543, ...  0.216331   3.410702   \n",
       "3   [2.85722, 2.41639, 2.46879, 2.47476, 2.77964, ...  0.046266   2.536589   \n",
       "4   [4.4348, 4.46372, 4.15406, 4.15757, 4.64013, 4...  0.255027   4.682148   \n",
       "5   [3.96834, 1.93799, 1.9347, 1.94431, 3.38007, 1...  0.223550   2.288528   \n",
       "6   [3.64415, 1.96741, 2.11442, 1.95557, 2.3897, 1...  0.162875   2.256876   \n",
       "7   [6.69585, 2.00216, 1.98863, 1.98551, 1.98065, ...  0.508531   2.747454   \n",
       "8   [4.66266, 4.32728, 4.29564, 4.07905, 3.99203, ...  0.099529   4.443819   \n",
       "9   [2.36805, 2.05706, 2.05694, 2.06412, 2.40571, ...  0.189731   2.471418   \n",
       "10  [1.54897, 1.01397, 1.16743, 1.01751, 1.27299, ...  0.076728   1.203748   \n",
       "11  [1.14799, 0.786602, 10.0463, 0.788984, 1.9944,...  0.861504   2.002010   \n",
       "12  [2.57335, 4.40686, 2.19681, 3.26529, 2.54339, ...  0.192145   2.783686   \n",
       "13  [1.45493, 1.00146, 1.04083, 1.00373, 1.03733, ...  0.049364   1.088257   \n",
       "\n",
       "    num_GPUs  batch_samples_per_GPU  \n",
       "0        1.0                     32  \n",
       "1        4.0                     32  \n",
       "2        8.0                      8  \n",
       "3        8.0                     32  \n",
       "4        8.0                      4  \n",
       "5        8.0                    128  \n",
       "6        8.0                     64  \n",
       "7        8.0                    256  \n",
       "8        4.0                     32  \n",
       "9        8.0                     32  \n",
       "10      16.0                     32  \n",
       "11      24.0                     21  \n",
       "12       8.0                     32  \n",
       "13      16.0                     32  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dir='/global/cscratch1/sd/vpa/proj/cosmogan/results_dir/128square/scaling_runs/'\n",
    "df=pd.DataFrame([])\n",
    "\n",
    "for fle in glob.glob(main_dir+'*scaling*'):\n",
    "#     print(fle)\n",
    "\n",
    "    info_dict=f_store_run_times(fle)\n",
    "    df=df.append(info_dict,ignore_index=True)\n",
    "    \n",
    "df=df.sort_values(by=['nodes','GPUs_per_node']).reset_index(drop=True)\n",
    "\n",
    "df['num_GPUs']=df.GPUs_per_node*df.nodes\n",
    "df['batch_samples_per_GPU']=(df.batchsize/df.num_GPUs).astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_strg</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>num_GPUs</th>\n",
       "      <th>batch_samples_per_GPU</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>train_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1_32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>356.19520</td>\n",
       "      <td>0.551521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_4_128</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "      <td>105.96690</td>\n",
       "      <td>0.444558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_8_64</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>77.87435</td>\n",
       "      <td>0.444776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_8_256</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>54.84403</td>\n",
       "      <td>0.497785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_8_32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>114.04490</td>\n",
       "      <td>0.628472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1_8_1024</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>128</td>\n",
       "      <td>45.10415</td>\n",
       "      <td>0.818162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1_8_512</td>\n",
       "      <td>512.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>64</td>\n",
       "      <td>47.06034</td>\n",
       "      <td>0.557007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1_8_2048</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>256</td>\n",
       "      <td>45.77109</td>\n",
       "      <td>1.889516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2_2_128</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "      <td>104.55030</td>\n",
       "      <td>0.307315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2_4_256</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>52.65502</td>\n",
       "      <td>0.408644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2_8_512</td>\n",
       "      <td>512.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32</td>\n",
       "      <td>25.85925</td>\n",
       "      <td>0.623332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3_8_512</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21</td>\n",
       "      <td>25.08310</td>\n",
       "      <td>1.101816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4_2_256</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>59.80198</td>\n",
       "      <td>0.690984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4_4_512</td>\n",
       "      <td>512.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32</td>\n",
       "      <td>25.42578</td>\n",
       "      <td>0.412116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    job_strg  batchsize  num_GPUs  batch_samples_per_GPU  train_mean  \\\n",
       "0     1_1_32       32.0       1.0                     32   356.19520   \n",
       "1    1_4_128      128.0       4.0                     32   105.96690   \n",
       "2     1_8_64       64.0       8.0                      8    77.87435   \n",
       "3    1_8_256      256.0       8.0                     32    54.84403   \n",
       "4     1_8_32       32.0       8.0                      4   114.04490   \n",
       "5   1_8_1024     1024.0       8.0                    128    45.10415   \n",
       "6    1_8_512      512.0       8.0                     64    47.06034   \n",
       "7   1_8_2048     2048.0       8.0                    256    45.77109   \n",
       "8    2_2_128      128.0       4.0                     32   104.55030   \n",
       "9    2_4_256      256.0       8.0                     32    52.65502   \n",
       "10   2_8_512      512.0      16.0                     32    25.85925   \n",
       "11   3_8_512      512.0      24.0                     21    25.08310   \n",
       "12   4_2_256      256.0       8.0                     32    59.80198   \n",
       "13   4_4_512      512.0      16.0                     32    25.42578   \n",
       "\n",
       "    train_err  \n",
       "0    0.551521  \n",
       "1    0.444558  \n",
       "2    0.444776  \n",
       "3    0.497785  \n",
       "4    0.628472  \n",
       "5    0.818162  \n",
       "6    0.557007  \n",
       "7    1.889516  \n",
       "8    0.307315  \n",
       "9    0.408644  \n",
       "10   0.623332  \n",
       "11   1.101816  \n",
       "12   0.690984  \n",
       "13   0.412116  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list=['job_strg','batchsize','num_GPUs','batch_samples_per_GPU','train_mean','train_err']\n",
    "df[col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers=itertools.cycle(('*','s','D','h','.','+'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b20bdae12e45acb89cbe2e770d447f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of Training time vs total number of GPUs ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Scaling plot \n",
    "df_temp=df[(df.batch_samples_per_GPU==32)].sort_values(by='num_GPUs')\n",
    "\n",
    "plt.figure()\n",
    "for i,j in df_temp.iterrows():\n",
    "    x,y,yerr=j.GPUs_per_node*j.nodes,j.train_mean,j.train_err\n",
    "    plt.errorbar(x=x,y=y,yerr=yerr,label=j.job_strg,markersize=10,marker=next(markers))\n",
    "    \n",
    "### Comparison with expected scaling\n",
    "x=np.linspace(1,16,num=50)\n",
    "plt.plot(x,356.0/x,color='y',label='y=356/x')\n",
    "plt.xlabel('total GPUs')\n",
    "plt.ylabel('Training time \\nin seconds')\n",
    "plt.legend()\n",
    "plt.title('Plot of Training time vs total number of GPUs ')\n",
    "\n",
    "# plt.savefig('scalingplot1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_strg</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>num_GPUs</th>\n",
       "      <th>batch_samples_per_GPU</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>train_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1_32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>356.19520</td>\n",
       "      <td>0.551521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_4_128</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "      <td>105.96690</td>\n",
       "      <td>0.444558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2_2_128</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "      <td>104.55030</td>\n",
       "      <td>0.307315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_8_256</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>54.84403</td>\n",
       "      <td>0.497785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2_4_256</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>52.65502</td>\n",
       "      <td>0.408644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4_2_256</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>59.80198</td>\n",
       "      <td>0.690984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2_8_512</td>\n",
       "      <td>512.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32</td>\n",
       "      <td>25.85925</td>\n",
       "      <td>0.623332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4_4_512</td>\n",
       "      <td>512.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32</td>\n",
       "      <td>25.42578</td>\n",
       "      <td>0.412116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_strg  batchsize  num_GPUs  batch_samples_per_GPU  train_mean  train_err\n",
       "0    1_1_32       32.0       1.0                     32   356.19520   0.551521\n",
       "1   1_4_128      128.0       4.0                     32   105.96690   0.444558\n",
       "8   2_2_128      128.0       4.0                     32   104.55030   0.307315\n",
       "3   1_8_256      256.0       8.0                     32    54.84403   0.497785\n",
       "9   2_4_256      256.0       8.0                     32    52.65502   0.408644\n",
       "12  4_2_256      256.0       8.0                     32    59.80198   0.690984\n",
       "10  2_8_512      512.0      16.0                     32    25.85925   0.623332\n",
       "13  4_4_512      512.0      16.0                     32    25.42578   0.412116"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp[col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f4e394a0b44821b5d2341c380b1bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2aaad8fc2250>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Variation with batch size\n",
    "df_temp=df[(df.nodes==1.0)&(df.GPUs_per_node==8.0)].sort_values(by='batchsize')\n",
    "plt.figure()\n",
    "for i,j in df_temp.iterrows():\n",
    "    x,y,yerr=j.batchsize,j.train_mean,j.train_err\n",
    "    plt.errorbar(x=x,y=y,yerr=yerr,label=j.job_strg,markersize=8,marker=next(markers))\n",
    "    \n",
    "# ### Comparison with expected scaling\n",
    "# x=np.linspace(16,1024,num=50)\n",
    "# plt.plot(x,(120*16)/x,color='y',label='y=350/x')\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('Training time \\nin seconds')\n",
    "# plt.xscale('log')\n",
    "plt.xticks([2**i for i in range(4,12)])\n",
    "plt.title('Training time vs batch-size')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_strg</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>num_GPUs</th>\n",
       "      <th>batch_samples_per_GPU</th>\n",
       "      <th>train_mean</th>\n",
       "      <th>train_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_8_32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>114.04490</td>\n",
       "      <td>0.628472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_8_64</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>77.87435</td>\n",
       "      <td>0.444776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_8_256</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>54.84403</td>\n",
       "      <td>0.497785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1_8_512</td>\n",
       "      <td>512.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>64</td>\n",
       "      <td>47.06034</td>\n",
       "      <td>0.557007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1_8_1024</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>128</td>\n",
       "      <td>45.10415</td>\n",
       "      <td>0.818162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1_8_2048</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>256</td>\n",
       "      <td>45.77109</td>\n",
       "      <td>1.889516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_strg  batchsize  num_GPUs  batch_samples_per_GPU  train_mean  train_err\n",
       "4    1_8_32       32.0       8.0                      4   114.04490   0.628472\n",
       "2    1_8_64       64.0       8.0                      8    77.87435   0.444776\n",
       "3   1_8_256      256.0       8.0                     32    54.84403   0.497785\n",
       "6   1_8_512      512.0       8.0                     64    47.06034   0.557007\n",
       "5  1_8_1024     1024.0       8.0                    128    45.10415   0.818162\n",
       "7  1_8_2048     2048.0       8.0                    256    45.77109   1.889516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp[col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot individual times\n",
    "# plt.figure()\n",
    "# y=df[df.job_strg=='2_8_512'].train_arr.values[0]\n",
    "# plt.plot(y,linestyle='',marker='*')\n",
    "# plt.axhline(np.mean(y),color='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3",
   "language": "python",
   "name": "v-jpt-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
